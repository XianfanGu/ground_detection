{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 149359,
     "status": "ok",
     "timestamp": 1600310436675,
     "user": {
      "displayName": "Xianfan Gu",
      "photoUrl": "",
      "userId": "02088240420816378259"
     },
     "user_tz": -480
    },
    "id": "sLFBEMfVdHId",
    "outputId": "3a5f2b91-a1a0-4f25-e502-a9f6766904d6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "annZipFile = \"cityscapes/gtFine_trainvaltest.zip\"\n",
    "unZipDir = \"cityscapes/gtFine_trainvaltest\"\n",
    "if not os.path.exists(unZipDir):\n",
    "  print(\"Unzipping \" + annZipFile)\n",
    "  with zipfile.ZipFile(annZipFile, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(unZipDir)\n",
    "  print(\"... done unzipping\")\n",
    "\n",
    "annZipFile = \"cityscapes/leftImg8bit_trainvaltest.zip\"\n",
    "unZipDir = \"cityscapes/leftImg8bit\"\n",
    "if not os.path.exists(unZipDir):\n",
    "  print(\"Unzipping \" + annZipFile)\n",
    "  with zipfile.ZipFile(annZipFile, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(unZipDir)\n",
    "  print(\"... done unzipping\")\n",
    "\n",
    "annZipFile = \"cityscapes/camera_trainvaltest.zip\"\n",
    "unZipDir = \"cityscapes/camera_trainvaltest\"\n",
    "if not os.path.exists(unZipDir):\n",
    "  print(\"Unzipping \" + annZipFile)\n",
    "  with zipfile.ZipFile(annZipFile, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(unZipDir)\n",
    "  print(\"... done unzipping\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "geohpotit_Ek"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 154770,
     "status": "ok",
     "timestamp": 1600310442097,
     "user": {
      "displayName": "Xianfan Gu",
      "photoUrl": "",
      "userId": "02088240420816378259"
     },
     "user_tz": -480
    },
    "id": "UzpXeiVddIdi",
    "outputId": "86ada397-3b63-4720-f137-9dcb999b8bb1"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, average_precision_score\n",
    "import math\n",
    "from PIL import Image\n",
    "import torch\n",
    "import cv2\n",
    "import json\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1654,
     "status": "error",
     "timestamp": 1600311216210,
     "user": {
      "displayName": "Xianfan Gu",
      "photoUrl": "",
      "userId": "02088240420816378259"
     },
     "user_tz": -480
    },
    "id": "XNge530E7rwL",
    "outputId": "85f0a7e5-a707-430a-ddc7-66b619b11039"
   },
   "outputs": [],
   "source": [
    "def recursive_glob(rootdir='.', suffix=''):\n",
    "    \"\"\"Performs recursive glob with given suffix and rootdir\n",
    "        :param rootdir is the root directory\n",
    "        :param suffix is the suffix to be searched\n",
    "    \"\"\"\n",
    "    \n",
    "    return [os.path.join(looproot, filename)\n",
    "        for looproot, _, filenames in os.walk(rootdir)\n",
    "        for filename in filenames if filename.endswith(suffix)]\n",
    "\n",
    "\n",
    "class Cityscapes(data.Dataset):\n",
    "    \"\"\"Cityscapes <http://www.cityscapes-dataset.com/> Dataset.\n",
    "    \n",
    "    **Parameters:**\n",
    "        - **root** (string): Root directory of dataset where directory 'leftImg8bit' and 'gtFine' or 'gtCoarse' are located.\n",
    "        - **split** (string, optional): The image split to use, 'train', 'test' or 'val' if mode=\"gtFine\" otherwise 'train', 'train_extra' or 'val'\n",
    "        - **mode** (string, optional): The quality mode to use, 'gtFine' or 'gtCoarse' or 'color'. Can also be a list to output a tuple with all specified target types.\n",
    "        - **transform** (callable, optional): A function/transform that takes in a PIL image and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "        - **target_transform** (callable, optional): A function/transform that takes in the target and transforms it.\n",
    "    \"\"\"\n",
    "\n",
    "    # Based on https://github.com/mcordts/cityscapesScripts\n",
    "    CityscapesClass = namedtuple('CityscapesClass', ['name', 'id', 'train_id', 'category', 'category_id',\n",
    "                                                     'has_instances', 'ignore_in_eval', 'color'])\n",
    "    classes = [\n",
    "        CityscapesClass('unlabeled',            0, 255, 'void', 0, False, True, (0, 0, 0)),\n",
    "        CityscapesClass('ego vehicle',          1, 255, 'void', 0, False, True, (0, 0, 0)),\n",
    "        CityscapesClass('rectification border', 2, 255, 'void', 0, False, True, (0, 0, 0)),\n",
    "        CityscapesClass('out of roi',           3, 255, 'void', 0, False, True, (0, 0, 0)),\n",
    "        CityscapesClass('static',               4, 255, 'void', 0, False, True, (0, 0, 0)),\n",
    "        CityscapesClass('dynamic',              5, 255, 'void', 0, False, True, (111, 74, 0)),\n",
    "        CityscapesClass('ground',               6, 255, 'void', 0, False, True, (81, 0, 81)),\n",
    "        CityscapesClass('road',                 7, 0, 'flat', 1, False, False, (128, 64, 128)),\n",
    "        CityscapesClass('sidewalk',             8, 1, 'flat', 1, False, False, (244, 35, 232)),\n",
    "        CityscapesClass('parking',              9, 255, 'flat', 1, False, True, (250, 170, 160)),\n",
    "        CityscapesClass('rail track',           10, 255, 'flat', 1, False, True, (230, 150, 140)),\n",
    "        CityscapesClass('building',             11, 2, 'construction', 2, False, False, (70, 70, 70)),\n",
    "        CityscapesClass('wall',                 12, 3, 'construction', 2, False, False, (102, 102, 156)),\n",
    "        CityscapesClass('fence',                13, 4, 'construction', 2, False, False, (190, 153, 153)),\n",
    "        CityscapesClass('guard rail',           14, 255, 'construction', 2, False, True, (180, 165, 180)),\n",
    "        CityscapesClass('bridge',               15, 255, 'construction', 2, False, True, (150, 100, 100)),\n",
    "        CityscapesClass('tunnel',               16, 255, 'construction', 2, False, True, (150, 120, 90)),\n",
    "        CityscapesClass('pole',                 17, 5, 'object', 3, False, False, (153, 153, 153)),\n",
    "        CityscapesClass('polegroup',            18, 255, 'object', 3, False, True, (153, 153, 153)),\n",
    "        CityscapesClass('traffic light',        19, 6, 'object', 3, False, False, (250, 170, 30)),\n",
    "        CityscapesClass('traffic sign',         20, 7, 'object', 3, False, False, (220, 220, 0)),\n",
    "        CityscapesClass('vegetation',           21, 8, 'nature', 4, False, False, (107, 142, 35)),\n",
    "        CityscapesClass('terrain',              22, 9, 'nature', 4, False, False, (152, 251, 152)),\n",
    "        CityscapesClass('sky',                  23, 10, 'sky', 5, False, False, (70, 130, 180)),\n",
    "        CityscapesClass('person',               24, 11, 'human', 6, True, False, (220, 20, 60)),\n",
    "        CityscapesClass('rider',                25, 12, 'human', 6, True, False, (255, 0, 0)),\n",
    "        CityscapesClass('car',                  26, 13, 'vehicle', 7, True, False, (0, 0, 142)),\n",
    "        CityscapesClass('truck',                27, 14, 'vehicle', 7, True, False, (0, 0, 70)),\n",
    "        CityscapesClass('bus',                  28, 15, 'vehicle', 7, True, False, (0, 60, 100)),\n",
    "        CityscapesClass('caravan',              29, 255, 'vehicle', 7, True, True, (0, 0, 90)),\n",
    "        CityscapesClass('trailer',              30, 255, 'vehicle', 7, True, True, (0, 0, 110)),\n",
    "        CityscapesClass('train',                31, 16, 'vehicle', 7, True, False, (0, 80, 100)),\n",
    "        CityscapesClass('motorcycle',           32, 17, 'vehicle', 7, True, False, (0, 0, 230)),\n",
    "        CityscapesClass('bicycle',              33, 18, 'vehicle', 7, True, False, (119, 11, 32)),\n",
    "        CityscapesClass('license plate',        -1, 255, 'vehicle', 7, False, True, (0, 0, 142)),\n",
    "    ]\n",
    "\n",
    "    train_id_to_color = [c.color for c in classes if (c.train_id != -1 and c.train_id != 255)]\n",
    "    train_id_to_color.append([0, 0, 0])\n",
    "    train_id_to_color = np.array(train_id_to_color)\n",
    "    id_to_train_id = np.array([c.train_id for c in classes])\n",
    "    \n",
    "    #train_id_to_color = [(0, 0, 0), (128, 64, 128), (70, 70, 70), (153, 153, 153), (107, 142, 35),\n",
    "    #                      (70, 130, 180), (220, 20, 60), (0, 0, 142)]\n",
    "    #train_id_to_color = np.array(train_id_to_color)\n",
    "    #id_to_train_id = np.array([c.category_id for c in classes], dtype='uint8') - 1\n",
    "\n",
    "    def __init__(self, root='cityscapes', split='train', mode='fine', target_type='semantic', transform=None):\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.mode = 'gtFine_trainvaltest'\n",
    "        self.camera = 'camera_trainvaltest'\n",
    "        self.target_type = target_type\n",
    "        self.images_dir = os.path.join(self.root, 'leftImg8bit', split)\n",
    "        self.targets_dir = os.path.join(self.root, self.mode, split)\n",
    "        self.param_dir = os.path.join(self.root, self.camera, split)\n",
    "        \n",
    "        self.transform = transform\n",
    "\n",
    "        self.split = split\n",
    "        self.images = []\n",
    "        self.targets = []\n",
    "        self.param = []\n",
    "\n",
    "        if split not in ['train', 'test', 'val']:\n",
    "            raise ValueError('Invalid split for mode! Please use split=\"train\", split=\"test\"'\n",
    "                             ' or split=\"val\"')\n",
    "\n",
    "        if not os.path.isdir(self.images_dir) or not os.path.isdir(self.targets_dir):\n",
    "            raise RuntimeError('Dataset not found or incomplete. Please make sure all required folders for the'\n",
    "                               ' specified \"split\" and \"mode\" are inside the \"root\" directory')\n",
    "        \n",
    "        for city in os.listdir(self.images_dir):\n",
    "            img_dir = os.path.join(self.images_dir, city)\n",
    "            target_dir = os.path.join(self.targets_dir, city)\n",
    "            param_dir = os.path.join(self.param_dir, city)\n",
    "            for file_name in os.listdir(img_dir):\n",
    "                self.images.append(os.path.join(img_dir, file_name))\n",
    "                target_name = '{}_{}'.format(file_name.split('_leftImg8bit')[0],\n",
    "                                             self._get_target_suffix(self.mode.split('_')[0], self.target_type))\n",
    "                param_name = '{}_{}'.format(file_name.split('_leftImg8bit')[0],'camera.json')\n",
    "                \n",
    "                self.targets.append(os.path.join(target_dir, target_name))\n",
    "                \n",
    "                self.param.append(os.path.join(param_dir, param_name))\n",
    "                \n",
    "    @classmethod\n",
    "    def encode_target(cls, target):\n",
    "        return cls.id_to_train_id[np.array(target)]\n",
    "\n",
    "    @classmethod\n",
    "    def decode_target(cls, target):\n",
    "        target[target == 255] = 19\n",
    "        #target = target.astype('uint8') + 1\n",
    "        return cls.train_id_to_color[target]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is a tuple of all target types if target_type is a list with more\n",
    "            than one item. Otherwise target is a json object if target_type=\"polygon\", else the image segmentation.\n",
    "        \"\"\"\n",
    "        image = Image.open(self.images[index]).convert('RGB')\n",
    "        target = Image.open(self.targets[index])\n",
    "        param = self._load_json(self.param[index])\n",
    "        if self.transform:\n",
    "            image, target = self.transform(image, target)\n",
    "        target = self.encode_target(target)\n",
    "        return image, target, param\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def _load_json(self, path):\n",
    "        with open(path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        return data\n",
    "\n",
    "    def _get_target_suffix(self, mode, target_type):\n",
    "        if target_type == 'instance':\n",
    "            return '{}_instanceIds.png'.format(mode)\n",
    "        elif target_type == 'semantic':\n",
    "            return '{}_labelIds.png'.format(mode)\n",
    "        elif target_type == 'color':\n",
    "            return '{}_color.png'.format(mode)\n",
    "        elif target_type == 'polygon':\n",
    "            return '{}_polygons.json'.format(mode)\n",
    "        elif target_type == 'depth':\n",
    "            return '{}_disparity.png'.format(mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 155354,
     "status": "ok",
     "timestamp": 1600310442694,
     "user": {
      "displayName": "Xianfan Gu",
      "photoUrl": "",
      "userId": "02088240420816378259"
     },
     "user_tz": -480
    },
    "id": "4oMHIGsCdckB"
   },
   "outputs": [],
   "source": [
    "##########\n",
    "#TODO: design your own network here. The expectation is to write from scratch. But it's okay to get some inspiration \n",
    "#from conference paper. The bottom line is that you will not just copy code from other repo\n",
    "##########\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from torchvision.models.utils import load_state_dict_from_url\n",
    "from collections import OrderedDict\n",
    "\n",
    "__all__ = ['MobileNetV2', 'mobilenet_v2']\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    'mobilenet_v2': 'https://download.pytorch.org/models/mobilenet_v2-b0353104.pth',\n",
    "}\n",
    "\n",
    "TOTAL_CLASSES = 1\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        return x\n",
    "\n",
    "class Vgg16(nn.Module):\n",
    "    def __init__(self, pretrained=True):   \n",
    "        super(Vgg16, self).__init__()\n",
    "        self.net = models.vgg16(pretrained).features.eval()\n",
    " \n",
    "    def forward(self, x):\n",
    "        out = []\n",
    "        for i in range(len(self.net)):\n",
    "            x = self.net[i](x)\n",
    "            if i in [3, 8, 15, 22, 29]:\n",
    "              out.append(x)\n",
    "        return out\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.net = models.resnet18(pretrained)\n",
    " \n",
    "    def forward(self, x):\n",
    "        out = {}\n",
    "        for name, module in self.net._modules.items():\n",
    "            if name == \"fc\":\n",
    "                x = x.view(x.size(0), -1)\n",
    "            x = module(x)\n",
    "            out[name] = x\n",
    "        return out\n",
    "\n",
    "\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    \"\"\"\n",
    "    This function is taken from the original tf repo.\n",
    "    It ensures that all layers have a channel number that is divisible by 8\n",
    "    It can be seen here:\n",
    "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    :param v:\n",
    "    :param divisor:\n",
    "    :param min_value:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "class _SimpleSegmentationModel(nn.Module):\n",
    "    def __init__(self, backbone, classifier):\n",
    "        super(_SimpleSegmentationModel, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.classifier = classifier\n",
    "        \n",
    "    def forward(self, x):\n",
    "        input_shape = x.shape[-2:]\n",
    "        features = self.backbone(x)\n",
    "        x = self.classifier(features)\n",
    "        x = F.interpolate(x, size=input_shape, mode='bilinear', align_corners=False)\n",
    "        return x\n",
    "\n",
    "class IntermediateLayerGetter(nn.ModuleDict):\n",
    "    \"\"\"\n",
    "    Module wrapper that returns intermediate layers from a model\n",
    "    It has a strong assumption that the modules have been registered\n",
    "    into the model in the same order as they are used.\n",
    "    This means that one should **not** reuse the same nn.Module\n",
    "    twice in the forward if you want this to work.\n",
    "    Additionally, it is only able to query submodules that are directly\n",
    "    assigned to the model. So if `model` is passed, `model.feature1` can\n",
    "    be returned, but not `model.feature1.layer2`.\n",
    "    Arguments:\n",
    "        model (nn.Module): model on which we will extract the features\n",
    "        return_layers (Dict[name, new_name]): a dict containing the names\n",
    "            of the modules for which the activations will be returned as\n",
    "            the key of the dict, and the value of the dict is the name\n",
    "            of the returned activation (which the user can specify).\n",
    "    Examples::\n",
    "        >>> m = torchvision.models.resnet18(pretrained=True)\n",
    "        >>> # extract layer1 and layer3, giving as names `feat1` and feat2`\n",
    "        >>> new_m = torchvision.models._utils.IntermediateLayerGetter(m,\n",
    "        >>>     {'layer1': 'feat1', 'layer3': 'feat2'})\n",
    "        >>> out = new_m(torch.rand(1, 3, 224, 224))\n",
    "        >>> print([(k, v.shape) for k, v in out.items()])\n",
    "        >>>     [('feat1', torch.Size([1, 64, 56, 56])),\n",
    "        >>>      ('feat2', torch.Size([1, 256, 14, 14]))]\n",
    "    \"\"\"\n",
    "    def __init__(self, model, return_layers):\n",
    "        if not set(return_layers).issubset([name for name, _ in model.named_children()]):\n",
    "            raise ValueError(\"return_layers are not present in model\")\n",
    "\n",
    "        orig_return_layers = return_layers\n",
    "        return_layers = {k: v for k, v in return_layers.items()}\n",
    "        layers = OrderedDict()\n",
    "        for name, module in model.named_children():\n",
    "            layers[name] = module\n",
    "            if name in return_layers:\n",
    "                del return_layers[name]\n",
    "            if not return_layers:\n",
    "                break\n",
    "\n",
    "        super(IntermediateLayerGetter, self).__init__(layers)\n",
    "        self.return_layers = orig_return_layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = OrderedDict()\n",
    "        for name, module in self.named_children():\n",
    "            x = module(x)\n",
    "            if name in self.return_layers:\n",
    "                out_name = self.return_layers[name]\n",
    "                print(out_name)\n",
    "                out[out_name] = x\n",
    "        return out\n",
    "\n",
    "class ConvBNReLU(nn.Sequential):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, dilation=1, groups=1):\n",
    "        #padding = (kernel_size - 1) // 2\n",
    "        super(ConvBNReLU, self).__init__(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size, stride, 0, dilation=dilation, groups=groups, bias=False),\n",
    "            nn.BatchNorm2d(out_planes),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "\n",
    "def fixed_padding(kernel_size, dilation):\n",
    "    kernel_size_effective = kernel_size + (kernel_size - 1) * (dilation - 1)\n",
    "    pad_total = kernel_size_effective - 1\n",
    "    pad_beg = pad_total // 2\n",
    "    pad_end = pad_total - pad_beg\n",
    "    return (pad_beg, pad_end, pad_beg, pad_end) \n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, dilation, expand_ratio):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.stride = stride\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = int(round(inp * expand_ratio))\n",
    "        self.use_res_connect = self.stride == 1 and inp == oup\n",
    "\n",
    "        layers = []\n",
    "        if expand_ratio != 1:\n",
    "            # pw\n",
    "            layers.append(ConvBNReLU(inp, hidden_dim, kernel_size=1))\n",
    "\n",
    "        layers.extend([\n",
    "            # dw\n",
    "            ConvBNReLU(hidden_dim, hidden_dim, stride=stride, dilation=dilation, groups=hidden_dim),\n",
    "            # pw-linear\n",
    "            nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(oup),\n",
    "        ])\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "\n",
    "        self.input_padding = fixed_padding( 3, dilation )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_pad = F.pad(x, self.input_padding)\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x_pad)\n",
    "        else:\n",
    "            return self.conv(x_pad)\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, num_classes=1000, output_stride=8, width_mult=1.0, inverted_residual_setting=None, round_nearest=8):\n",
    "        \"\"\"\n",
    "        MobileNet V2 main class\n",
    "        Args:\n",
    "            num_classes (int): Number of classes\n",
    "            width_mult (float): Width multiplier - adjusts number of channels in each layer by this amount\n",
    "            inverted_residual_setting: Network structure\n",
    "            round_nearest (int): Round the number of channels in each layer to be a multiple of this number\n",
    "            Set to 1 to turn off rounding\n",
    "        \"\"\"\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        block = InvertedResidual\n",
    "        input_channel = 32\n",
    "        last_channel = 1280\n",
    "        self.output_stride = output_stride\n",
    "        current_stride = 1\n",
    "        if inverted_residual_setting is None:\n",
    "            inverted_residual_setting = [\n",
    "                # t, c, n, s\n",
    "                [1, 16, 1, 1],\n",
    "                [6, 24, 2, 2],\n",
    "                [6, 32, 3, 2],\n",
    "                [6, 64, 4, 2],\n",
    "                [6, 96, 3, 1],\n",
    "                [6, 160, 3, 2],\n",
    "                [6, 320, 1, 1],\n",
    "            ]\n",
    "\n",
    "        # only check the first element, assuming user knows t,c,n,s are required\n",
    "        if len(inverted_residual_setting) == 0 or len(inverted_residual_setting[0]) != 4:\n",
    "            raise ValueError(\"inverted_residual_setting should be non-empty \"\n",
    "                             \"or a 4-element list, got {}\".format(inverted_residual_setting))\n",
    "\n",
    "        # building first layer\n",
    "        input_channel = _make_divisible(input_channel * width_mult, round_nearest)\n",
    "        self.last_channel = _make_divisible(last_channel * max(1.0, width_mult), round_nearest)\n",
    "        features = [ConvBNReLU(3, input_channel, stride=2)]\n",
    "        current_stride *= 2\n",
    "        dilation=1\n",
    "        previous_dilation = 1\n",
    "\n",
    "        # building inverted residual blocks\n",
    "        for t, c, n, s in inverted_residual_setting:\n",
    "            output_channel = _make_divisible(c * width_mult, round_nearest)\n",
    "            previous_dilation = dilation\n",
    "            if current_stride == output_stride:\n",
    "                stride = 1\n",
    "                dilation *= s\n",
    "            else:\n",
    "                stride = s\n",
    "                current_stride *= s\n",
    "            output_channel = int(c * width_mult)\n",
    "\n",
    "            for i in range(n):\n",
    "                if i==0:\n",
    "                    features.append(block(input_channel, output_channel, stride, previous_dilation, expand_ratio=t))\n",
    "                else:\n",
    "                    features.append(block(input_channel, output_channel, 1, dilation, expand_ratio=t))\n",
    "                input_channel = output_channel\n",
    "        # building last several layers\n",
    "        features.append(ConvBNReLU(input_channel, self.last_channel, kernel_size=1))\n",
    "        # make it nn.Sequential\n",
    "        self.features = nn.Sequential(*features)\n",
    "\n",
    "        # building classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.last_channel, num_classes),\n",
    "        )\n",
    "\n",
    "        # weight initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.mean([2, 3])\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def mobilenet_v2(pretrained=False, progress=True, **kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a MobileNetV2 architecture from\n",
    "    `\"MobileNetV2: Inverted Residuals and Linear Bottlenecks\" <https://arxiv.org/abs/1801.04381>`_.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    model = MobileNetV2(**kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(model_urls['mobilenet_v2'], progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "class AtrousSeparableConvolution(nn.Module):\n",
    "    \"\"\" Atrous Separable Convolution\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size,\n",
    "                            stride=1, padding=0, dilation=1, bias=True):\n",
    "        super(AtrousSeparableConvolution, self).__init__()\n",
    "        self.body = nn.Sequential(\n",
    "            # Separable Conv\n",
    "            nn.Conv2d( in_channels, in_channels, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, bias=bias, groups=in_channels ),\n",
    "            # PointWise Conv\n",
    "            nn.Conv2d( in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=bias),\n",
    "        )\n",
    "        \n",
    "        self._init_weight()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.body(x)\n",
    "\n",
    "    def _init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "class ASPPConv(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, dilation):\n",
    "        modules = [\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=dilation, dilation=dilation, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        super(ASPPConv, self).__init__(*modules)\n",
    "\n",
    "class ASPPPooling(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ASPPPooling, self).__init__(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        size = x.shape[-2:]\n",
    "        x = super(ASPPPooling, self).forward(x)\n",
    "        return F.interpolate(x, size=size, mode='bilinear', align_corners=False)\n",
    "\n",
    "class ASPP(nn.Module):\n",
    "    def __init__(self, in_channels, atrous_rates):\n",
    "        super(ASPP, self).__init__()\n",
    "        out_channels = 256\n",
    "        modules = []\n",
    "        modules.append(nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)))\n",
    "\n",
    "        rate1, rate2, rate3 = tuple(atrous_rates)\n",
    "        modules.append(ASPPConv(in_channels, out_channels, rate1))\n",
    "        modules.append(ASPPConv(in_channels, out_channels, rate2))\n",
    "        modules.append(ASPPConv(in_channels, out_channels, rate3))\n",
    "        modules.append(ASPPPooling(in_channels, out_channels))\n",
    "\n",
    "        self.convs = nn.ModuleList(modules)\n",
    "\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv2d(5 * out_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.1),)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = []\n",
    "        for conv in self.convs:\n",
    "            res.append(conv(x))\n",
    "        res = torch.cat(res, dim=1)\n",
    "        return self.project(res)\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_separable_conv(module):\n",
    "    new_module = module\n",
    "    if isinstance(module, nn.Conv2d) and module.kernel_size[0]>1:\n",
    "        new_module = AtrousSeparableConvolution(module.in_channels,\n",
    "                                      module.out_channels, \n",
    "                                      module.kernel_size,\n",
    "                                      module.stride,\n",
    "                                      module.padding,\n",
    "                                      module.dilation,\n",
    "                                      module.bias)\n",
    "    for name, child in module.named_children():\n",
    "        new_module.add_module(name, convert_to_separable_conv(child))\n",
    "    return new_module\n",
    "\n",
    "\n",
    "class DeepLabHeadV3Plus(nn.Module):\n",
    "    def __init__(self, in_channels, low_level_channels, num_classes, aspp_dilate=[12, 24, 36]):\n",
    "        super(DeepLabHeadV3Plus, self).__init__()\n",
    "        self.project = nn.Sequential( \n",
    "            nn.Conv2d(low_level_channels, 48, 1, bias=False),\n",
    "            nn.BatchNorm2d(48),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.aspp = ASPP(in_channels, aspp_dilate)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv2d(304, 256, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, num_classes, 1)\n",
    "        )\n",
    "        self._init_weight()\n",
    "\n",
    "    def forward(self, feature):\n",
    "        low_level_feature = self.project( feature['low_level'] )\n",
    "        output_feature = self.aspp(feature['out'])\n",
    "        output_feature = F.interpolate(output_feature, size=low_level_feature.shape[2:], mode='bilinear', align_corners=False)\n",
    "        return self.classifier( torch.cat( [ low_level_feature, output_feature ], dim=1 ) )\n",
    "    \n",
    "    def _init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "class DeepLabV3(_SimpleSegmentationModel):\n",
    "    \"\"\"\n",
    "    Implements DeepLabV3 model from\n",
    "    `\"Rethinking Atrous Convolution for Semantic Image Segmentation\"\n",
    "    <https://arxiv.org/abs/1706.05587>`_.\n",
    "    Arguments:\n",
    "        backbone (nn.Module): the network used to compute the features for the model.\n",
    "            The backbone should return an OrderedDict[Tensor], with the key being\n",
    "            \"out\" for the last feature map used, and \"aux\" if an auxiliary classifier\n",
    "            is used.\n",
    "        classifier (nn.Module): module that takes the \"out\" element returned from\n",
    "            the backbone and returns a dense prediction.\n",
    "        aux_classifier (nn.Module, optional): auxiliary classifier used during training\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 155351,
     "status": "ok",
     "timestamp": 1600310442696,
     "user": {
      "displayName": "Xianfan Gu",
      "photoUrl": "",
      "userId": "02088240420816378259"
     },
     "user_tz": -480
    },
    "id": "D6Dtn7yudjC8"
   },
   "outputs": [],
   "source": [
    "def segmentation_eval(gts, preds, classes, plot_file_name):\n",
    "    \"\"\"\n",
    "    @param    gts               numpy.ndarray   ground truth labels\n",
    "    @param    preds             numpy.ndarray   predicted labels\n",
    "    @param    classes           string          class names\n",
    "    @param    plot_file_name    string          plot file names\n",
    "    \"\"\"\n",
    "    ious, counts = compute_confusion_matrix(gts, preds)\n",
    "    aps = compute_ap(gts, preds)\n",
    "    plot_results(counts, ious, aps, classes, plot_file_name)\n",
    "    for i in range(len(classes)):\n",
    "        print('{:>20s}: AP: {:0.2f}, IoU: {:0.2f}'.format(classes[i], aps[i], ious[i]))\n",
    "    print('{:>20s}: AP: {:0.2f}, IoU: {:0.2f}'.format('mean', np.mean(aps), np.mean(ious)))\n",
    "    return aps, ious\n",
    "\n",
    "def plot_results(counts, ious, aps, classes, file_name):\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "    conf = counts / np.sum(counts, 1, keepdims=True)\n",
    "    conf = np.concatenate([conf, np.array(aps).reshape(-1,1), \n",
    "                           np.array(ious).reshape(-1,1)], 1)\n",
    "    conf = conf * 100.\n",
    "    sns.heatmap(conf, annot=True, ax=ax, fmt='3.0f') \n",
    "    arts = [] \n",
    "    # labels, title and ticks\n",
    "    _ = ax.set_xlabel('Predicted labels')\n",
    "    arts.append(_)\n",
    "    _ = ax.set_ylabel('True labels')\n",
    "    arts.append(_)\n",
    "    _ = ax.set_title('Confusion Matrix, mAP: {:5.1f}, mIoU: {:5.1f}'.format(\n",
    "      np.mean(aps)*100., np.mean(ious)*100.))\n",
    "    arts.append(_)\n",
    "    _ = ax.xaxis.set_ticklabels(classes + ['AP', 'IoU'], rotation=90)\n",
    "    arts.append(_)\n",
    "    _ = ax.yaxis.set_ticklabels(classes, rotation=0)\n",
    "    arts.append(_)\n",
    "    fig.savefig(file_name, bbox_inches='tight')\n",
    "\n",
    "def compute_ap(gts, preds):\n",
    "    aps = []\n",
    "    for i in range(preds.shape[1]):\n",
    "      ap, prec, rec = calc_pr(gts == i, preds[:,i:i+1,:,:])\n",
    "      aps.append(ap)\n",
    "    return aps\n",
    "\n",
    "def calc_pr(gt, out, wt=None):\n",
    "    gt = gt.astype(np.float64).reshape((-1,1))\n",
    "    out = out.astype(np.float64).reshape((-1,1))\n",
    "\n",
    "    tog = np.concatenate([gt, out], axis=1)*1.\n",
    "    ind = np.argsort(tog[:,1], axis=0)[::-1]\n",
    "    tog = tog[ind,:]\n",
    "    cumsumsortgt = np.cumsum(tog[:,0])\n",
    "    cumsumsortwt = np.cumsum(tog[:,0]-tog[:,0]+1)\n",
    "    prec = cumsumsortgt / cumsumsortwt\n",
    "    rec = cumsumsortgt / np.sum(tog[:,0])\n",
    "    ap = voc_ap(rec, prec)\n",
    "    return ap, rec, prec\n",
    "\n",
    "def voc_ap(rec, prec):\n",
    "    rec = rec.reshape((-1,1))\n",
    "    prec = prec.reshape((-1,1))\n",
    "    z = np.zeros((1,1)) \n",
    "    o = np.ones((1,1))\n",
    "    mrec = np.vstack((z, rec, o))\n",
    "    mpre = np.vstack((z, prec, z))\n",
    "\n",
    "    mpre = np.maximum.accumulate(mpre[::-1])[::-1]\n",
    "    I = np.where(mrec[1:] != mrec[0:-1])[0]+1;\n",
    "    ap = np.sum((mrec[I] - mrec[I-1])*mpre[I])\n",
    "    return ap\n",
    "\n",
    "def compute_confusion_matrix(gts, preds):\n",
    "    preds_cls = np.argmax(preds, 1)\n",
    "    gts = gts[:,0,:,:]\n",
    "    conf = confusion_matrix(gts.ravel(), preds_cls.ravel())\n",
    "    inter = np.diag(conf)\n",
    "    union = np.sum(conf, 0) + np.sum(conf, 1) - np.diag(conf)\n",
    "    union = np.maximum(union, 1)\n",
    "    return inter / union, conf\n",
    "\n",
    "def camera_calibration(param, pos):\n",
    "    extrinsic_param = param['extrinsic']\n",
    "    intrinsic_param = param['intrinsic']\n",
    "    pitch = extrinsic_param['pitch'].numpy()[0]\n",
    "    roll = extrinsic_param['roll'].numpy()[0]\n",
    "    yaw = extrinsic_param['yaw'].numpy()[0]\n",
    "    x = extrinsic_param['x'].numpy()[0]\n",
    "    y = extrinsic_param['y'].numpy()[0]\n",
    "    z = extrinsic_param['z'].numpy()[0]\n",
    "    fx = intrinsic_param['fx'].numpy()[0]\n",
    "    fy = intrinsic_param['fy'].numpy()[0]\n",
    "    u0 = intrinsic_param['u0'].numpy()[0]\n",
    "    v0 = intrinsic_param['v0'].numpy()[0]\n",
    "    print(yaw)\n",
    "    cy = np.cos(yaw)\n",
    "    cr = np.cos(roll)\n",
    "    cp = np.cos(pitch)\n",
    "    sy = np.sin(yaw)\n",
    "    sr = np.sin(roll)\n",
    "    sp = np.sin(pitch)\n",
    "    \n",
    "    R = np.array([[cy*cp,cy*sp*sr-sy*cr,cy*sp*cr+sy*sr],\n",
    "                  [sy*cp,sy*sp*sr+cy*cr,sy*sp*cr-cy*sr],\n",
    "                  [-sp,cp*sr,cp*cr]])\n",
    "    t = np.array([[x],\n",
    "                  [y],\n",
    "                  [z]])\n",
    "    K = np.array([[fx,0,u0],\n",
    "                  [0,fy,v0],\n",
    "                  [0,0,1]])\n",
    "    intrinsic_rot = np.array([[0,-1,0],\n",
    "                              [0,0,-1],\n",
    "                              [1,0,0]])\n",
    "    R = R.T\n",
    "    t = np.matmul(-1*R, t)\n",
    "    Rt = np.concatenate((R,t),axis = 1)\n",
    "    K = np.matmul(K, intrinsic_rot)\n",
    "    KRt = np.matmul(K, Rt)\n",
    "    pos = pos-KRt[:,2].ravel()\n",
    "    KRt = np.delete(KRt,[2],1)\n",
    "    world_pos = np.linalg.solve(KRt, pos)\n",
    "    world_z = 1/world_pos[2]\n",
    "    world_y = world_z*world_pos[1]\n",
    "    world_x = world_z*world_pos[0]\n",
    "    return [world_x,world_y,world_z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 155347,
     "status": "ok",
     "timestamp": 1600310442698,
     "user": {
      "displayName": "Xianfan Gu",
      "photoUrl": "",
      "userId": "02088240420816378259"
     },
     "user_tz": -480
    },
    "id": "aeZky2AHeyec"
   },
   "outputs": [],
   "source": [
    "# Colab has GPUs, you will have to move tensors and models to GPU.\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1673,
     "status": "ok",
     "timestamp": 1600311206673,
     "user": {
      "displayName": "Xianfan Gu",
      "photoUrl": "",
      "userId": "02088240420816378259"
     },
     "user_tz": -480
    },
    "id": "yUEu8F7Je27N",
    "outputId": "b6d82172-a664-49cf-f5d1-c299981414eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepLabV3(\n",
      "  (backbone): IntermediateLayerGetter(\n",
      "    (low_level_features): Sequential(\n",
      "      (0): ConvBNReLU(\n",
      "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "      )\n",
      "      (1): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): ConvBNReLU(\n",
      "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), groups=32, bias=False)\n",
      "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (2): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): ConvBNReLU(\n",
      "            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): ConvBNReLU(\n",
      "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), groups=96, bias=False)\n",
      "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (3): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): ConvBNReLU(\n",
      "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): ConvBNReLU(\n",
      "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (high_level_features): Sequential(\n",
      "      (4): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): ConvBNReLU(\n",
      "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): ConvBNReLU(\n",
      "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), groups=144, bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (5): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): ConvBNReLU(\n",
      "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): ConvBNReLU(\n",
      "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), groups=192, bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (6): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): ConvBNReLU(\n",
      "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): ConvBNReLU(\n",
      "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), groups=192, bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (7): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): ConvBNReLU(\n",
      "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): ConvBNReLU(\n",
      "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), groups=192, bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (8): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): ConvBNReLU(\n",
      "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): ConvBNReLU(\n",
      "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), dilation=(2, 2), groups=384, bias=False)\n",
      "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (9): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): ConvBNReLU(\n",
      "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): ConvBNReLU(\n",
      "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), dilation=(2, 2), groups=384, bias=False)\n",
      "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (10): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): ConvBNReLU(\n",
      "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): ConvBNReLU(\n",
      "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), dilation=(2, 2), groups=384, bias=False)\n",
      "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (11): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): ConvBNReLU(\n",
      "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): ConvBNReLU(\n",
      "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), dilation=(2, 2), groups=384, bias=False)\n",
      "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (12): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): ConvBNReLU(\n",
      "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): ConvBNReLU(\n",
      "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), dilation=(2, 2), groups=576, bias=False)\n",
      "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (13): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): ConvBNReLU(\n",
      "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): ConvBNReLU(\n",
      "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), dilation=(2, 2), groups=576, bias=False)\n",
      "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (14): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): ConvBNReLU(\n",
      "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): ConvBNReLU(\n",
      "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), dilation=(2, 2), groups=576, bias=False)\n",
      "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (15): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): ConvBNReLU(\n",
      "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): ConvBNReLU(\n",
      "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), dilation=(4, 4), groups=960, bias=False)\n",
      "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (16): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): ConvBNReLU(\n",
      "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): ConvBNReLU(\n",
      "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), dilation=(4, 4), groups=960, bias=False)\n",
      "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (17): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): ConvBNReLU(\n",
      "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): ConvBNReLU(\n",
      "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), dilation=(4, 4), groups=960, bias=False)\n",
      "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): DeepLabHeadV3Plus(\n",
      "    (project): Sequential(\n",
      "      (0): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (aspp): ASPP(\n",
      "      (convs): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): ASPPConv(\n",
      "          (0): Conv2d(320, 256, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): ASPPConv(\n",
      "          (0): Conv2d(320, 256, kernel_size=(3, 3), stride=(1, 1), padding=(24, 24), dilation=(24, 24), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): ASPPConv(\n",
      "          (0): Conv2d(320, 256, kernel_size=(3, 3), stride=(1, 1), padding=(36, 36), dilation=(36, 36), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (4): ASPPPooling(\n",
      "          (0): AdaptiveAvgPool2d(output_size=1)\n",
      "          (1): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (project): Sequential(\n",
      "        (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (classifier): Sequential(\n",
      "      (0): Conv2d(304, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(256, 19, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#############\n",
    "#TODO: initialize your model \n",
    "num_classes = 19\n",
    "output_stride = 8\n",
    "if output_stride==8:\n",
    "    replace_stride_with_dilation=[False, True, True]\n",
    "    aspp_dilate = [12, 24, 36]\n",
    "else:\n",
    "    replace_stride_with_dilation=[False, False, True]\n",
    "    aspp_dilate = [6, 12, 18]\n",
    "\n",
    "backbone = mobilenet_v2(pretrained=True, output_stride=output_stride)\n",
    "\n",
    "\n",
    "backbone.low_level_features = backbone.features[0:4]\n",
    "backbone.high_level_features = backbone.features[4:-1]\n",
    "backbone.features = None\n",
    "backbone.classifier = None\n",
    "\n",
    "inplanes = 320\n",
    "low_level_planes = 24\n",
    "\n",
    "return_layers = {'high_level_features': 'out', 'low_level_features': 'low_level'}\n",
    "classifier = DeepLabHeadV3Plus(inplanes, low_level_planes, num_classes, aspp_dilate)\n",
    "\n",
    "backbone = IntermediateLayerGetter(backbone, return_layers=return_layers)\n",
    "\n",
    "\n",
    "#print(backbone.low_level_features)\n",
    "#resnet18 = ResNet18(pretrained=True)\n",
    "save_dir = 'new_models'\n",
    "parentModelName = \"deeplab_mobilenetv2_v3_plus\" \n",
    "parentEpoch = 1669\n",
    "IS_GPU = True\n",
    "model = DeepLabV3(backbone, classifier)\n",
    "checkpoint = torch.load(os.path.join(save_dir, parentModelName+'_epoch-'+str(parentEpoch)+'.pth'), map_location=torch.device('cpu'))\n",
    "model.load_state_dict(checkpoint[\"model_state\"])\n",
    "if IS_GPU:\n",
    "  model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 167230,
     "status": "ok",
     "timestamp": 1600310454596,
     "user": {
      "displayName": "Xianfan Gu",
      "photoUrl": "",
      "userId": "02088240420816378259"
     },
     "user_tz": -480
    },
    "id": "okJ-P_JGesKq"
   },
   "outputs": [],
   "source": [
    "# This is a trivial semantic segmentor. For eqch pixel location it computes the \n",
    "# distribution of the class label in the training set and uses that as the \n",
    "# prediction. Quite unsuprisingly it doesn't perform very well. Though we provide\n",
    "# this code so that you can understand the data formats for the benchmarking \n",
    "# functions.\n",
    "\n",
    "def simple_train():\n",
    "    train_dataset = SegmentationDataset(split='train')\n",
    "    train_dataloader = data.DataLoader(train_dataset, batch_size=1, \n",
    "                                       shuffle=True, num_workers=4, \n",
    "                                       drop_last=True)\n",
    "    counts = np.zeros((train_dataset.n_classes, 224, 288))\n",
    "    N = 0\n",
    "    for i, batch in enumerate(tqdm(train_dataloader)):\n",
    "      img, gt = batch\n",
    "      gt = gt.cpu().numpy()\n",
    "      for j in range(train_dataset.n_classes):\n",
    "          counts[j,:,:] += gt[0,0,:,:] == j\n",
    "      N += 1\n",
    "    model = counts / N\n",
    "    return model\n",
    "\n",
    "def simple_predict(split, model):\n",
    "    dataset = SegmentationDataset(split=split, data_dir=DATASET_PATH)\n",
    "    dataloader = data.DataLoader(dataset, batch_size=1, shuffle=False, \n",
    "                                 num_workers=0, drop_last=False)\n",
    "    gts, preds = [], []\n",
    "    for i, batch in enumerate(tqdm(dataloader)):\n",
    "      img, gt = batch\n",
    "      gt = gt.cpu().numpy()\n",
    "      gts.append(gt[0,:,:,:])\n",
    "      preds.append(model)\n",
    "\n",
    "    gts = np.array(gts)\n",
    "    preds = np.array(preds)\n",
    "    return gts, preds, list(dataset.classes)\n",
    "\n",
    "def predict(split, model):\n",
    "    dataset = SegmentationDataset(split=split, data_dir=DATASET_PATH)\n",
    "    dataloader = data.DataLoader(dataset, batch_size=1, shuffle=False, \n",
    "                                 num_workers=0, drop_last=False)\n",
    "    gts, preds = [], []\n",
    "    with torch.no_grad():\n",
    "      for i, batch in enumerate(tqdm(dataloader)):\n",
    "        img, gt = batch\n",
    "        if IS_GPU:\n",
    "            images = img.cuda()\n",
    "        outputs = model(Variable(images))\n",
    "        outputs = outputs.cpu().numpy()\n",
    "        gt = gt.cpu().numpy()\n",
    "        gts.append(gt[0,:,:,:])\n",
    "        preds.append(outputs[0,:,:,:])\n",
    "    gts = np.array(gts)\n",
    "    preds = np.array(preds)\n",
    "    print(preds.shape)\n",
    "    return gts, preds, list(dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 167225,
     "status": "ok",
     "timestamp": 1600310454598,
     "user": {
      "displayName": "Xianfan Gu",
      "photoUrl": "",
      "userId": "02088240420816378259"
     },
     "user_tz": -480
    },
    "id": "ByJ4fRSD5mX7"
   },
   "outputs": [],
   "source": [
    "def class_balanced_cross_entropy_loss(logit, target, ignore_index=255, weight=None, size_average=True, batch_average=True):\n",
    "    \"\"\"Define the class balanced cross entropy loss to train the network\n",
    "    Args:\n",
    "    output: Output of the network\n",
    "    label: Ground truth label\n",
    "    Returns:\n",
    "    Tensor that evaluates the loss\n",
    "    \"\"\"\n",
    "\n",
    "    n, c, h, w = logit.size()\n",
    "    # logit = logit.permute(0, 2, 3, 1)\n",
    "    target = target.squeeze(1)\n",
    "    if weight is None:\n",
    "        criterion = nn.CrossEntropyLoss(weight=weight, ignore_index=ignore_index, size_average=False)\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss(weight=torch.from_numpy(np.array(weight)).float().cuda(), ignore_index=ignore_index, size_average=False)\n",
    "    loss = criterion(logit, target.long())\n",
    "\n",
    "    if size_average:\n",
    "        loss /= (h * w)\n",
    "\n",
    "    if batch_average:\n",
    "        loss /= n\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 183846,
     "status": "ok",
     "timestamp": 1600310471225,
     "user": {
      "displayName": "Xianfan Gu",
      "photoUrl": "",
      "userId": "02088240420816378259"
     },
     "user_tz": -480
    },
    "id": "3sJiJPb3huXK",
    "outputId": "f372e350-6a5b-40f7-e644-edbff70b7ea2"
   },
   "outputs": [],
   "source": [
    "def lr_poly(base_lr, iter_, max_iter=100, power=0.9):\n",
    "    return base_lr * ((1 - float(iter_) / max_iter) ** power)\n",
    "########################################################################\n",
    "# TODO: Implement your training cycles, make sure you evaluate on validation \n",
    "# dataset and compute evaluation metrics every so often. \n",
    "# You may also want to save models that perform well.\n",
    "from collections import OrderedDict\n",
    "from dataloaders import custom_transforms as tr\n",
    "from dataloaders import ext_transforms as et\n",
    "import timeit\n",
    "\n",
    "testBatch = 4  # Testing Batch\n",
    "EPOCHS = 800\n",
    "snapshot = 10\n",
    "nEpochs = EPOCHS\n",
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Tune the learning rate.\n",
    "# See whether the momentum is useful or not\n",
    "# Use the following optimizer\n",
    "p = OrderedDict()  # Parameters to include in report\n",
    "p['trainBatch'] = 4  # Training batch size\n",
    "testBatch = 4  # Testing batch size\n",
    "useTest = True  # See evolution of the test set when training\n",
    "nValInterval = 5  # Run on test set every nTestInterval epochs\n",
    "snapshot = 10  # Store a model every snapshot epochs\n",
    "p['nAveGrad'] = 1  # Average the gradient of several iterations\n",
    "p['lr'] = 1e-7  # Learning rate\n",
    "p['wd'] = 5e-4  # Weight decay\n",
    "p['momentum'] = 0.9  # Momentum\n",
    "p['epoch_size'] = 10  # How many epochs to change learning rate\n",
    "#learning_rate = 1e-8\n",
    "#     wd = 0.0002\n",
    "# Setting of parameters\n",
    "# Parameters in p are used for the name of the model\n",
    "optimizer = optim.SGD(model.parameters(), lr=p['lr'], momentum=p['momentum'], weight_decay=p['wd'])\n",
    "p['optimizer'] = str(optimizer)\n",
    "\"\"\"\n",
    "optimizer = optim.SGD([\n",
    "    {'params': [pr[1] for pr in model.stages.named_parameters() if 'weight' in pr[0]], 'weight_decay': wd,\n",
    "     'initial_lr': lr},\n",
    "    {'params': [pr[1] for pr in model.stages.named_parameters() if 'bias' in pr[0]], 'lr': 2 * lr, 'initial_lr': 2 * lr},\n",
    "    {'params': [pr[1] for pr in model.side_prep.named_parameters() if 'weight' in pr[0]], 'weight_decay': wd,\n",
    "     'initial_lr': lr},\n",
    "    {'params': [pr[1] for pr in model.side_prep.named_parameters() if 'bias' in pr[0]], 'lr': 2 * lr,\n",
    "     'initial_lr': 2 * lr},\n",
    "    {'params': [pr[1] for pr in model.score_dsn.named_parameters() if 'weight' in pr[0]], 'lr': lr / 10,\n",
    "     'weight_decay': wd, 'initial_lr': lr / 10},\n",
    "    {'params': [pr[1] for pr in model.score_dsn.named_parameters() if 'bias' in pr[0]], 'lr': 2 * lr / 10,\n",
    "     'initial_lr': 2 * lr / 10},\n",
    "    {'params': [pr[1] for pr in model.upscale.named_parameters() if 'weight' in pr[0]], 'lr': 0, 'initial_lr': 0},\n",
    "    {'params': [pr[1] for pr in model.upscale_.named_parameters() if 'weight' in pr[0]], 'lr': 0, 'initial_lr': 0},\n",
    "    {'params': net.fuse.weight, 'lr': lr / 100, 'initial_lr': lr / 100, 'weight_decay': wd},\n",
    "    {'params': net.fuse.bias, 'lr': 2 * lr / 100, 'initial_lr': 2 * lr / 100},\n",
    "], lr=lr, momentum=0.9)\n",
    "\"\"\"\n",
    "#optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=wd)\n",
    "\n",
    "composed_transforms_tr = et.ExtCompose([\n",
    "            #et.ExtResize( 512 ),\n",
    "            et.ExtRandomCrop(size=(513, 513)),\n",
    "            et.ExtColorJitter( brightness=0.5, contrast=0.5, saturation=0.5 ),\n",
    "            et.ExtRandomHorizontalFlip(),\n",
    "            et.ExtToTensor(),\n",
    "            et.ExtNormalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "composed_transforms_ts = et.ExtCompose([\n",
    "            et.ExtResize( 512 ),\n",
    "            #et.ExtRandomCrop(size=(513, 1024)),\n",
    "            et.ExtToTensor(),\n",
    "            et.ExtNormalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "cityscapes_train = Cityscapes(split='train', transform=composed_transforms_tr)\n",
    "cityscapes_val = Cityscapes(split='val', transform=composed_transforms_ts)\n",
    "#cityscapes_test = CityscapesSegmentation(split='test', transform=composed_transforms_ts)\n",
    "\n",
    "\n",
    "train_dataloader = data.DataLoader(cityscapes_train, batch_size=p['trainBatch'], shuffle=True, num_workers=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([-1, 18, 17, 16, 255, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0])\n",
      "(119, 11, 32)\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "Label = namedtuple( 'Label' , [\n",
    "\n",
    "    'name'        , # The identifier of this label, e.g. 'car', 'person', ... .\n",
    "                    # We use them to uniquely name a class\n",
    "\n",
    "    'id'          , # An integer ID that is associated with this label.\n",
    "                    # The IDs are used to represent the label in ground truth images\n",
    "                    # An ID of -1 means that this label does not have an ID and thus\n",
    "                    # is ignored when creating ground truth images (e.g. license plate).\n",
    "                    # Do not modify these IDs, since exactly these IDs are expected by the\n",
    "                    # evaluation server.\n",
    "\n",
    "    'trainId'     , # Feel free to modify these IDs as suitable for your method. Then create\n",
    "                    # ground truth images with train IDs, using the tools provided in the\n",
    "                    # 'preparation' folder. However, make sure to validate or submit results\n",
    "                    # to our evaluation server using the regular IDs above!\n",
    "                    # For trainIds, multiple labels might have the same ID. Then, these labels\n",
    "                    # are mapped to the same class in the ground truth images. For the inverse\n",
    "                    # mapping, we use the label that is defined first in the list below.\n",
    "                    # For example, mapping all void-type classes to the same ID in training,\n",
    "                    # might make sense for some approaches.\n",
    "                    # Max value is 255!\n",
    "\n",
    "    'category'    , # The name of the category that this label belongs to\n",
    "\n",
    "    'categoryId'  , # The ID of this category. Used to create ground truth images\n",
    "                    # on category level.\n",
    "\n",
    "    'hasInstances', # Whether this label distinguishes between single instances or not\n",
    "\n",
    "    'ignoreInEval', # Whether pixels having this class as ground truth label are ignored\n",
    "                    # during evaluations or not\n",
    "\n",
    "    'color'       , # The color of this label\n",
    "    ] )\n",
    "labels = [\n",
    "    #       name                     id    trainId   category            catId     hasInstances   ignoreInEval   color\n",
    "    Label(  'unlabeled'            ,  0 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'ego vehicle'          ,  1 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'rectification border' ,  2 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'out of roi'           ,  3 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'static'               ,  4 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'dynamic'              ,  5 ,      255 , 'void'            , 0       , False        , True         , (111, 74,  0) ),\n",
    "    Label(  'ground'               ,  6 ,      255 , 'void'            , 0       , False        , True         , ( 81,  0, 81) ),\n",
    "    Label(  'road'                 ,  7 ,        0 , 'flat'            , 1       , False        , False        , (128, 64,128) ),\n",
    "    Label(  'sidewalk'             ,  8 ,        1 , 'flat'            , 1       , False        , False        , (244, 35,232) ),\n",
    "    Label(  'parking'              ,  9 ,      255 , 'flat'            , 1       , False        , True         , (250,170,160) ),\n",
    "    Label(  'rail track'           , 10 ,      255 , 'flat'            , 1       , False        , True         , (230,150,140) ),\n",
    "    Label(  'building'             , 11 ,        2 , 'construction'    , 2       , False        , False        , ( 70, 70, 70) ),\n",
    "    Label(  'wall'                 , 12 ,        3 , 'construction'    , 2       , False        , False        , (102,102,156) ),\n",
    "    Label(  'fence'                , 13 ,        4 , 'construction'    , 2       , False        , False        , (190,153,153) ),\n",
    "    Label(  'guard rail'           , 14 ,      255 , 'construction'    , 2       , False        , True         , (180,165,180) ),\n",
    "    Label(  'bridge'               , 15 ,      255 , 'construction'    , 2       , False        , True         , (150,100,100) ),\n",
    "    Label(  'tunnel'               , 16 ,      255 , 'construction'    , 2       , False        , True         , (150,120, 90) ),\n",
    "    Label(  'pole'                 , 17 ,        5 , 'object'          , 3       , False        , False        , (153,153,153) ),\n",
    "    Label(  'polegroup'            , 18 ,      255 , 'object'          , 3       , False        , True         , (153,153,153) ),\n",
    "    Label(  'traffic light'        , 19 ,        6 , 'object'          , 3       , False        , False        , (250,170, 30) ),\n",
    "    Label(  'traffic sign'         , 20 ,        7 , 'object'          , 3       , False        , False        , (220,220,  0) ),\n",
    "    Label(  'vegetation'           , 21 ,        8 , 'nature'          , 4       , False        , False        , (107,142, 35) ),\n",
    "    Label(  'terrain'              , 22 ,        9 , 'nature'          , 4       , False        , False        , (152,251,152) ),\n",
    "    Label(  'sky'                  , 23 ,       10 , 'sky'             , 5       , False        , False        , ( 70,130,180) ),\n",
    "    Label(  'person'               , 24 ,       11 , 'human'           , 6       , True         , False        , (220, 20, 60) ),\n",
    "    Label(  'rider'                , 25 ,       12 , 'human'           , 6       , True         , False        , (255,  0,  0) ),\n",
    "    Label(  'car'                  , 26 ,       13 , 'vehicle'         , 7       , True         , False        , (  0,  0,142) ),\n",
    "    Label(  'truck'                , 27 ,       14 , 'vehicle'         , 7       , True         , False        , (  0,  0, 70) ),\n",
    "    Label(  'bus'                  , 28 ,       15 , 'vehicle'         , 7       , True         , False        , (  0, 60,100) ),\n",
    "    Label(  'caravan'              , 29 ,      255 , 'vehicle'         , 7       , True         , True         , (  0,  0, 90) ),\n",
    "    Label(  'trailer'              , 30 ,      255 , 'vehicle'         , 7       , True         , True         , (  0,  0,110) ),\n",
    "    Label(  'train'                , 31 ,       16 , 'vehicle'         , 7       , True         , False        , (  0, 80,100) ),\n",
    "    Label(  'motorcycle'           , 32 ,       17 , 'vehicle'         , 7       , True         , False        , (  0,  0,230) ),\n",
    "    Label(  'bicycle'              , 33 ,       18 , 'vehicle'         , 7       , True         , False        , (119, 11, 32) ),\n",
    "    Label(  'license plate'        , -1 ,       -1 , 'vehicle'         , 7       , False        , True         , (  0,  0,142) ),\n",
    "]\n",
    "trainId2label   = { label.trainId : label for label in reversed(labels) }\n",
    "print(trainId2label.keys())\n",
    "print(trainId2label[18].color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2214,
     "status": "error",
     "timestamp": 1600311227763,
     "user": {
      "displayName": "Xianfan Gu",
      "photoUrl": "",
      "userId": "02088240420816378259"
     },
     "user_tz": -480
    },
    "id": "qW1yCEj55lZG",
    "outputId": "2f86bc9c-0436-4be9-d12a-979cb397e171",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "low_level\n",
      "out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 19, 512, 1024])\n",
      "(3, 512, 1024)\n",
      "-0.0195\n",
      "tensor(0.8920)\n",
      "x= 2.7914914526574828 , y= 0.04252697821087524 , z 1.0963148748070226\n",
      "-0.0195\n",
      "tensor(0.3179)\n",
      "x= 2.7719743493410833 , y= -0.11038873365214485 , z 1.080399714494495\n",
      "-0.0195\n",
      "tensor(6.6655)\n",
      "有障碍物\n",
      "-0.0195\n",
      "tensor(10.6083)\n",
      "有障碍物\n",
      "-0.0195\n",
      "tensor(1.6569)\n",
      "有障碍物\n",
      "-0.0195\n",
      "tensor(1.7688)\n",
      "有障碍物\n",
      "-0.0195\n",
      "tensor(-0.3467)\n",
      "x= 2.85477680483211 , y= -0.36924524771345935 , z 1.1649579611000922\n",
      "-0.0195\n",
      "tensor(-1.2685)\n",
      "x= 2.8470957167785707 , y= -0.3343331751798014 , z 1.1569093983408905\n",
      "-0.0195\n",
      "tensor(0.6071)\n",
      "x= 2.848294640541646 , y= -0.37949233339506233 , z 1.1589110947518142\n",
      "-0.0195\n",
      "tensor(0.6269)\n",
      "x= 2.78697198378091 , y= 0.47234975745845265 , z 1.083896354748006\n",
      "-0.0195\n",
      "tensor(-1.2655)\n",
      "x= 2.80951429637067 , y= 0.5817722287280898 , z 1.1035397779782605\n",
      "-0.0195\n",
      "tensor(0.9317)\n",
      "x= 2.843100495357365 , y= 0.5072400047518244 , z 1.1372662880338853\n",
      "-0.0195\n",
      "tensor(0.4700)\n",
      "x= 2.8393470883922687 , y= 0.41747394069853716 , z 1.135338601956996\n",
      "-0.0195\n",
      "tensor(14.6862)\n",
      "有障碍物\n",
      "-0.0195\n",
      "tensor(13.1510)\n",
      "有障碍物\n",
      "-0.0195\n",
      "tensor(-0.6537)\n",
      "x= 2.7729753781060515 , y= 0.03362634639001215 , z 1.078659840841995\n",
      "-0.0195\n",
      "tensor(-5.3287)\n",
      "x= 2.6996776043249255 , y= -0.32906267637947423 , z 1.0149174184468142\n",
      "-0.0195\n",
      "tensor(-3.1257)\n",
      "x= 2.6723970731221733 , y= -0.2924095634572937 , z 0.9879712959974717\n",
      "-0.0195\n",
      "tensor(1.1728)\n",
      "有障碍物\n",
      "-0.0195\n",
      "tensor(-2.5313)\n",
      "x= 2.7023862334626765 , y= -0.10837634399916767 , z 1.0133819086428764\n",
      "-0.0195\n",
      "tensor(0.2619)\n",
      "x= 2.748743856380396 , y= -0.03053890172853614 , z 1.0565409703520012\n",
      "-0.0195\n",
      "tensor(-2.0095)\n",
      "x= 2.6690078991340473 , y= -0.15748704969163688 , z 0.9821764410192291\n",
      "-0.0195\n",
      "tensor(-2.3217)\n",
      "x= 2.7021704544646292 , y= 0.04341892000671455 , z 1.010324785796122\n",
      "-0.0195\n",
      "tensor(-1.3798)\n",
      "x= 2.76787033883324 , y= -0.22813735579875274 , z 1.078659840841995\n",
      "-0.0195\n",
      "tensor(0.9918)\n",
      "x= 2.7812729646148036 , y= -0.19605828634606443 , z 1.090957977456129\n",
      "-0.0195\n",
      "tensor(-1.0454)\n",
      "x= 2.758651738098053 , y= -0.15091455402297388 , z 1.0683371520927694\n",
      "-0.0195\n",
      "tensor(-0.9434)\n",
      "x= 2.7278465264234617 , y= -0.05647675456344259 , z 1.0369137426936796\n",
      "-0.0195\n",
      "tensor(-1.2043)\n",
      "x= 2.759874888143118 , y= 0.09277551637806927 , z 1.064940021701771\n",
      "-0.0195\n",
      "tensor(-2.1960)\n",
      "x= 2.7349210933970354 , y= 0.13471763570396367 , z 1.040134149397931\n",
      "-0.0195\n",
      "tensor(-6.6322)\n",
      "x= 2.7228645023575253 , y= 0.28024375579185046 , z 1.0257976765512573\n",
      "-0.0195\n",
      "tensor(-1.1696)\n",
      "x= 2.717492275976569 , y= 0.4200487335424738 , z 1.0180024404200028\n",
      "-0.0195\n",
      "tensor(-0.7960)\n",
      "x= 2.735985885250742 , y= 0.4462545596781537 , z 1.035311005918632\n",
      "-0.0195\n",
      "tensor(-1.6755)\n",
      "x= 2.7768808205885405 , y= 0.4183609184370768 , z 1.0751968509003365\n",
      "-0.0195\n",
      "tensor(0.3797)\n",
      "x= 2.8168902022955598 , y= 0.3730401720582002 , z 1.1145574654396282\n",
      "-0.0195\n",
      "tensor(-0.9095)\n",
      "x= 2.8166522353413326 , y= 0.3608382853325276 , z 1.1145574654396282\n",
      "-0.0195\n",
      "tensor(1.4485)\n",
      "有障碍物\n",
      "-0.0195\n",
      "tensor(1.4485)\n",
      "有障碍物\n",
      "-0.0195\n",
      "tensor(0.7124)\n",
      "x= 2.7813894287519765 , y= -0.19008652352711553 , z 1.090957977456129\n",
      "-0.0195\n",
      "tensor(-0.1911)\n",
      "x= 2.772240842776882 , y= -0.18971040411685067 , z 1.0821452100324072\n",
      "-0.0195\n",
      "tensor(-0.1530)\n",
      "x= 2.7387765769771275 , y= -0.18871805886356435 , z 1.049916520084039\n",
      "-0.0195\n",
      "tensor(-3.0615)\n",
      "x= 2.7099194153582515 , y= -0.21665433673128878 , z 1.0226652999753194\n",
      "-0.0195\n",
      "tensor(-1.9494)\n",
      "x= 2.702422470730581 , y= -0.27036628721189593 , z 1.0164575886228413\n",
      "-0.0195\n",
      "tensor(-3.2355)\n",
      "x= 2.708354593470116 , y= -0.29689144144576907 , z 1.0226652999753192\n",
      "-0.0195\n",
      "tensor(-3.8917)\n",
      "x= 2.741388548005921 , y= -0.3190449127349981 , z 1.054877033193301\n",
      "-0.0195\n",
      "tensor(-3.0203)\n",
      "x= 2.7880658579905733 , y= -0.3249332394449797 , z 1.0999154621483909\n",
      "-0.0195\n",
      "tensor(0.9143)\n",
      "x= 2.8014828086696686 , y= -0.12205687088752501 , z 1.1090212582925396\n",
      "-0.0195\n",
      "tensor(-1.1016)\n",
      "x= 2.812819611752213 , y= 0.262953543063695 , z 1.1127059317904384\n",
      "-0.0195\n",
      "tensor(-0.2174)\n",
      "x= 2.8359693746406442 , y= 0.346624213043379 , z 1.1334174397446906\n",
      "-0.0195\n",
      "tensor(2.5606)\n",
      "有障碍物\n",
      "-0.0195\n",
      "tensor(-0.0965)\n",
      "x= 2.815472495031475 , y= 0.39898159525770593 , z 1.1127059317904384\n",
      "-0.0195\n",
      "tensor(-0.4939)\n",
      "x= 2.7076631288995583 , y= 0.4061206292150045 , z 1.008803130893351\n",
      "-0.0195\n",
      "tensor(-0.2299)\n",
      "x= 2.7025287340861324 , y= 0.4646717781794446 , z 1.0027620722358503\n",
      "-0.0195\n",
      "tensor(-0.3843)\n",
      "x= 2.698407282486814 , y= 0.4921890308887289 , z 0.9982785468531102\n",
      "-0.0195\n",
      "tensor(-0.3843)\n",
      "x= 2.698407282486814 , y= 0.4921890308887289 , z 0.9982785468531102\n",
      "-0.0195\n",
      "tensor(0.0103)\n",
      "x= 2.8688725534312574 , y= 0.5686472157328643 , z 1.160919729879165\n",
      "-0.0195\n",
      "tensor(0.9744)\n",
      "x= 2.877824602523837 , y= -0.3979190340656045 , z 1.187680203957736\n",
      "-0.0195\n",
      "tensor(0.9744)\n",
      "x= 2.877824602523837 , y= -0.3979190340656045 , z 1.187680203957736\n",
      "-0.0195\n",
      "tensor(3.3020)\n",
      "有障碍物\n",
      "-0.0195\n",
      "tensor(9.0733)\n",
      "有障碍物\n",
      "-0.0195\n",
      "tensor(11.9312)\n",
      "有障碍物\n",
      "-0.0195\n",
      "tensor(1.2486)\n",
      "有障碍物\n",
      "-0.0195\n",
      "tensor(1.2486)\n",
      "有障碍物\n",
      "-0.0195\n",
      "tensor(1.3249)\n",
      "有障碍物\n",
      "-0.0195\n",
      "tensor(-0.3467)\n",
      "x= 2.85477680483211 , y= -0.36924524771345935 , z 1.1649579611000922\n",
      "-0.0195\n",
      "tensor(3.1104)\n",
      "有障碍物\n",
      "-0.0195\n",
      "tensor(3.1802)\n",
      "有障碍物\n",
      "-0.0195\n",
      "tensor(4.0168)\n",
      "有障碍物\n",
      "-0.0195\n",
      "tensor(1.6384)\n",
      "有障碍物\n",
      "-0.0195\n",
      "tensor(0.0520)\n",
      "x= 2.8189172837672363 , y= 0.5756150361663478 , z 1.1127059317904384\n",
      "-0.0195\n",
      "tensor(0.9316)\n",
      "x= 2.7679087423477977 , y= 0.5047158183188504 , z 1.064940021701771\n",
      "-0.0195\n",
      "tensor(-2.3758)\n",
      "x= 2.7841012389065014 , y= 0.4184379438984964 , z 1.0821452100324072\n",
      "-0.0195\n",
      "tensor(1.3224)\n",
      "有障碍物\n",
      "-0.0195\n",
      "tensor(-1.1857)\n",
      "x= 2.7848328867196916 , y= 0.4559535887435987 , z 1.082145210032407\n",
      "-0.0195\n",
      "tensor(1.0298)\n",
      "有障碍物\n",
      "-0.0195\n",
      "tensor(0.3553)\n",
      "x= 2.7393218768322622 , y= 0.5319281619115437 , z 1.0369137426936796\n",
      "-0.0195\n",
      "tensor(-0.9465)\n",
      "x= 2.7365225465344873 , y= 0.5588898307092691 , z 1.0337132161332967\n"
     ]
    }
   ],
   "source": [
    "%matplotlib tk\n",
    "import timeit\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "temp = []\n",
    "def __onclick__(event, param,image_pixel):\n",
    "    global ix, iy\n",
    "    ix, iy = event.xdata, event.ydata\n",
    "    global coords\n",
    "    if ix and iy:\n",
    "        coords = np.array([ix, iy, 0.5])\n",
    "        row = int(image_pixel.shape[0]-1-iy)\n",
    "        column = int(ix)\n",
    "        world_coords = camera_calibration(param, coords*2)\n",
    "        for p in temp:\n",
    "          p.remove()\n",
    "          temp.pop()\n",
    "        points = plt.plot([ix], [iy], 'go')\n",
    "        temp.extend(points)\n",
    "        print(image_pixel[row,column])\n",
    "        if(image_pixel[row,column]<1.0):\n",
    "            print(\"x=\",world_coords[0],\", y=\",world_coords[1], \", z\", world_coords[2])\n",
    "        else:\n",
    "            print(\"有障碍物\")\n",
    "    \n",
    "def imshow(img):\n",
    "    img = img / 2.0 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    fig = plt.figure()\n",
    "    coords = fig.canvas.callbacks.connect('button_press_event', __onclick__)\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "subset_indices = []\n",
    "for i in range(0,1): \n",
    "  n = random.randint(1,400)\n",
    "  subset_indices.append(n)\n",
    "\n",
    "subset = torch.utils.data.Subset(cityscapes_val, subset_indices)\n",
    "testloader_1 = torch.utils.data.DataLoader(subset, batch_size=1, num_workers=0, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "for i, data in enumerate(testloader_1):\n",
    "  \n",
    "  img, gt_label, param = data\n",
    "  if IS_GPU:\n",
    "    img = img.cuda()\n",
    "  out = model(img)\n",
    "  if IS_GPU:\n",
    "    img = img.cpu() \n",
    "    out = out.detach().cpu()\n",
    "  img = torchvision.utils.make_grid(img)\n",
    "  out_img = img.numpy().copy()\n",
    "  alpha = 0.4\n",
    "  print(out.shape)\n",
    "  for id in range(out.shape[1]):\n",
    "  #out = torch.sum(out,dim=1)\n",
    "    #print()\n",
    "    if (id == 0):\n",
    "        Img = np.ones(out_img.shape, out_img.dtype)\n",
    "        Img[0,:,:] =  trainId2label[id].color[0]*Img[0,:,:]\n",
    "        Img[1,:,:] =  trainId2label[id].color[1]*Img[1,:,:]\n",
    "        Img[2,:,:] =  trainId2label[id].color[2]*Img[2,:,:]\n",
    "        Img = Img/255.0\n",
    "        mask_id = torchvision.utils.make_grid(out[:,id,:,:])\n",
    "        mask_id = mask_id.detach().numpy()\n",
    "       # mask_id = (mask_id - mask_id.mean()) / mask_id.std()\n",
    "        #mask_id[mask_id<(mask_id.mean()-mask_id.std())]=0\n",
    "        #mask_id = mask_id.clip(0,1)\n",
    "        mask = Img*mask_id\n",
    "        out_img = cv2.addWeighted(mask, alpha, out_img, 1.0, 0, out_img)\n",
    "  #imshow(torchvision.utils.make_grid(gt_label))\n",
    "  #imshow(img)\n",
    "  #imshow(torch.from_numpy(out_img))\n",
    "  print(out_img.shape)\n",
    "  plot_img = torch.from_numpy(out_img) / 2.0 + 0.5     # unnormalize\n",
    "  npimg = plot_img.numpy()\n",
    "  fig = plt.figure()\n",
    "  ground_indicator = out[0,0,:,:]\n",
    "  coords = fig.canvas.callbacks.connect('button_press_event', lambda event: __onclick__(event,param,ground_indicator))\n",
    "  plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "deeplab_v3plus_best_model.ipynb",
   "provenance": [
    {
     "file_id": "1assXCSrnwBb37fbTisNRf8IuB3G-261C",
     "timestamp": 1587752147335
    },
    {
     "file_id": "1uxqAlJvHhJQ87yQlzLMgApUuBCfHvM_W",
     "timestamp": 1586648932535
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
